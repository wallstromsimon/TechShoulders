---
id: a-mathematical-theory-of-communication
type: work
name: A Mathematical Theory of Communication
kind: paper
era: 1940s
year: 1948
domains:
  - Computing
  - Mathematics
  - Information Theory
  - Telecommunications
edges: []
links:
  - label: Wikipedia
    url: https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication
  - label: Original Paper (PDF)
    url: https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf
  - label: Bell System Technical Journal
    url: https://ieeexplore.ieee.org/document/6773024
---

"A Mathematical Theory of Communication" is a landmark paper by Claude Shannon published in the Bell System Technical Journal in July and October 1948. Often called the "Magna Carta of the Information Age," this paper founded the field of information theory and unified the understanding of all forms of communication.

## Background

Shannon wrote the paper while working at Bell Labs, where he had been since 1941. The telecommunications industry was grappling with fundamental questions: How much information could be transmitted through a channel? How could signals be protected from noise? Shannon's paper provided rigorous mathematical answers.

Remarkably, Shannon was initially not planning to publish the paper, and did so only at the urging of colleagues at Bell Laboratories<sup><a href="#source-1">[1]</a></sup>.

## Key Contributions

### Information Entropy

Shannon introduced the concept of information entropy—a measure of the uncertainty or information content in a message. He showed that information could be quantified using the formula:

H = -Σ p(x) log₂ p(x)

This measure, now called Shannon entropy, became fundamental to both information theory and thermodynamics.

### The Bit

The paper introduced and formalized the term "bit" (binary digit) as the fundamental unit of information<sup><a href="#source-2">[2]</a></sup>. Shannon credited John Tukey with coining the term, but it was Shannon who gave it precise mathematical meaning.

### Channel Capacity

Shannon proved that every communication channel has a maximum rate at which information can be transmitted reliably—the channel capacity. This theorem established fundamental limits that engineers had never known existed.

### Source Coding Theorem

Shannon demonstrated that data could be compressed to eliminate redundancy, up to a limit determined by the entropy of the source. This principle underlies all modern data compression.

### Noisy Channel Coding Theorem

Perhaps the most surprising result: Shannon proved that reliable communication is possible over noisy channels, as long as the transmission rate stays below the channel capacity. This theorem suggested that error-correcting codes could achieve arbitrarily low error rates—a result that seemed almost magical to engineers of the time.

## Impact

Historian James Gleick rated the paper as the most important development of 1948, placing the transistor second in the same time period, emphasizing that Shannon's paper was "even more profound and more fundamental" than the transistor<sup><a href="#source-3">[3]</a></sup>.

The paper has tens of thousands of citations, being one of the most influential and cited scientific papers of all time. It gave rise to:

- Modern telecommunications and data compression
- Error-correcting codes used in everything from CDs to space probes
- Cryptography and secure communications
- The theoretical foundations of the digital age

Scientific American called it the "Magna Carta of the Information Age."

---

## Sources

1. <span id="source-1"></span>Wikipedia. ["A Mathematical Theory of
   Communication."](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication) Notes that
   Shannon published at colleagues' urging.
2. <span id="source-2"></span>IEEE Information Theory Society. ["Claude E.
   Shannon."](https://www.itsoc.org/about/shannon) Documents Shannon's introduction of the bit.
3. <span id="source-3"></span>Quanta Magazine. ["How Claude Shannon Invented the
   Future."](https://www.quantamagazine.org/how-claude-shannons-information-theory-invented-the-future-20201222/)
   James Gleick's assessment of the paper's importance.
